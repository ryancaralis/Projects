{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Cortez and Morais, 2007] P. Cortez and A. Morais. A Data Mining Approach to Predict Forest Fires using Meteorological Data. In J. Neves, M. F. Santos and J. Machado Eds., New Trends in Artificial Intelligence, Proceedings of the 13th EPIA 2007 - Portuguese Conference on Artificial Intelligence, December, Guimar√£es, Portugal, pp. 512-523, 2007. APPIA, ISBN-13 978-989-95618-0-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/ryanc/Desktop/forestfires.csv')\n",
    "df['area_t'] = np.log(df['area'] + 1)\n",
    "df = df.join(pd.get_dummies(df['month']))\n",
    "df = df.join(pd.get_dummies(df['day']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s = df.iloc[:, [0,1]]\n",
    "df_t = df.iloc[:, 14:33]\n",
    "df_fwi = df.iloc[:, 4:8]\n",
    "df_m = df.iloc[:, 8:12]\n",
    "df_st = df_s.join(df_t)\n",
    "df_stfwi = df_st.join(df_fwi)\n",
    "df_stm = df_st.join(df_m)\n",
    "feature_list = [df_stfwi, df_stm, df_fwi, df_m]\n",
    "features_names = ['STFWI', 'STM', 'FWI', 'M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_results(y_true, y_pred, model):\n",
    "\n",
    "    # Regression metrics\n",
    "    explained_variance = metrics.explained_variance_score(y_true, y_pred)\n",
    "    mean_absolute_error = metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mse = metrics.mean_squared_error(y_true, y_pred) \n",
    "    median_absolute_error = metrics.median_absolute_error(y_true, y_pred)\n",
    "    r2 = metrics.r2_score(y_true, y_pred)\n",
    "    \n",
    "    print('explained_variance: ', round(explained_variance, 4))    \n",
    "    print('r2: ', round(r2, 4))\n",
    "    print('MAE: ', round(mean_absolute_error, 4))\n",
    "    print('MSE: ', round(mse, 4))\n",
    "    print('RMSE: ', round(np.sqrt(mse), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.LinearRegression()\n",
    "dt = tree.DecisionTreeRegressor()\n",
    "svm_func = svm.SVR()\n",
    "rf = RandomForestRegressor()\n",
    "nn = MLPRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression: STFWI\n",
      "explained_variance:  0.0016\n",
      "r2:  -0.03\n",
      "MAE:  18.2437\n",
      "MSE:  8475.4839\n",
      "RMSE:  92.0624\n",
      "\n",
      "\n",
      "explained_variance:  0.0024\n",
      "r2:  -0.0262\n",
      "MAE:  12.7956\n",
      "MSE:  4150.1836\n",
      "RMSE:  64.4219\n",
      "\n",
      "\n",
      "Linear Regression: STM\n",
      "explained_variance:  0.0018\n",
      "r2:  -0.0297\n",
      "MAE:  18.2368\n",
      "MSE:  8473.0466\n",
      "RMSE:  92.0492\n",
      "\n",
      "\n",
      "explained_variance:  0.0021\n",
      "r2:  -0.0266\n",
      "MAE:  12.7996\n",
      "MSE:  4151.6442\n",
      "RMSE:  64.4333\n",
      "\n",
      "\n",
      "Linear Regression: FWI\n",
      "explained_variance:  0.0001\n",
      "r2:  -0.0325\n",
      "MAE:  18.1445\n",
      "MSE:  8496.2113\n",
      "RMSE:  92.1749\n",
      "\n",
      "\n",
      "explained_variance:  0.0001\n",
      "r2:  -0.0299\n",
      "MAE:  12.9386\n",
      "MSE:  4164.9944\n",
      "RMSE:  64.5368\n",
      "\n",
      "\n",
      "Linear Regression: M\n",
      "explained_variance:  -0.0001\n",
      "r2:  -0.0327\n",
      "MAE:  18.1701\n",
      "MSE:  8497.4948\n",
      "RMSE:  92.1819\n",
      "\n",
      "\n",
      "explained_variance:  -0.0002\n",
      "r2:  -0.0301\n",
      "MAE:  12.9253\n",
      "MSE:  4165.8529\n",
      "RMSE:  64.5434\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, features in enumerate(feature_list):\n",
    "    print('Linear Regression: ' + features_names[idx])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, df['area_t'], test_size=0.3, random_state = 0 )\n",
    "    model = reg.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = (np.exp(y_pred))-1\n",
    "    y_pred[y_pred < 0] = 0\n",
    "    y_test = (np.exp(y_test))-1\n",
    "    y_test[y_test < 0] = 0\n",
    "    y_true = y_test\n",
    "    regression_results(y_true, y_pred, 'reg')\n",
    "    print('\\n')\n",
    "    y_pred = model.predict(features)\n",
    "    y_pred = (np.exp(y_pred))-1\n",
    "    y_pred[y_pred < 0] = 0\n",
    "    y_true = df['area']\n",
    "    regression_results(y_true, y_pred, 'reg')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree: STFWI\n",
      "explained_variance:  -0.0\n",
      "r2:  -0.0317\n",
      "MAE:  17.689\n",
      "MSE:  8439.5853\n",
      "RMSE:  91.8672\n",
      "\n",
      "\n",
      "explained_variance:  -0.0002\n",
      "r2:  -0.0301\n",
      "MAE:  12.802\n",
      "MSE:  4166.1244\n",
      "RMSE:  64.5455\n",
      "\n",
      "\n",
      "Decision Tree: STM\n",
      "explained_variance:  -0.0002\n",
      "r2:  -0.0319\n",
      "MAE:  17.7426\n",
      "MSE:  8441.1844\n",
      "RMSE:  91.8759\n",
      "\n",
      "\n",
      "explained_variance:  -0.0004\n",
      "r2:  -0.0303\n",
      "MAE:  12.8123\n",
      "MSE:  4166.5941\n",
      "RMSE:  64.5492\n",
      "\n",
      "\n",
      "Decision Tree: FWI\n",
      "explained_variance:  -0.0002\n",
      "r2:  -0.0314\n",
      "MAE:  17.7629\n",
      "MSE:  8437.6471\n",
      "RMSE:  91.8567\n",
      "\n",
      "\n",
      "explained_variance:  -0.0002\n",
      "r2:  -0.0303\n",
      "MAE:  12.9175\n",
      "MSE:  4166.7345\n",
      "RMSE:  64.5502\n",
      "\n",
      "\n",
      "Decision Tree: M\n",
      "explained_variance:  -0.0002\n",
      "r2:  -0.0319\n",
      "MAE:  17.7426\n",
      "MSE:  8441.1844\n",
      "RMSE:  91.8759\n",
      "\n",
      "\n",
      "explained_variance:  -0.0004\n",
      "r2:  -0.0303\n",
      "MAE:  12.8123\n",
      "MSE:  4166.5941\n",
      "RMSE:  64.5492\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 10, shuffle = True, random_state = 0)\n",
    "df_target = df['area_t'].round().astype(int)\n",
    "\n",
    "for idx, features in enumerate(feature_list):\n",
    "    print('Decision Tree: ' + features_names[idx])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, df_target, test_size=0.3, random_state = 0 )\n",
    "    parameters = {'max_leaf_nodes': list(range(2, 10)), 'min_samples_split': list(range(2, 10))}\n",
    "    grid = GridSearchCV(dt, parameters, cv=kf, scoring = 'neg_mean_squared_error')\n",
    "    grid.fit(X_train, y_train)\n",
    "    model = grid.best_estimator_\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = (np.exp(y_pred))-1\n",
    "    y_pred[y_pred < 0] = 0\n",
    "    y_test = (np.exp(y_test))-1\n",
    "    y_test[y_test < 0] = 0\n",
    "    y_true = y_test\n",
    "    regression_results(y_true, y_pred, 'dt')\n",
    "    print('\\n')\n",
    "    y_pred = model.predict(features)\n",
    "    y_pred = (np.exp(y_pred))-1\n",
    "    y_pred[y_pred < 0] = 0\n",
    "    y_true = df['area']\n",
    "    regression_results(y_true, y_pred, 'dt')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: STFWI\n",
      "explained_variance:  -0.0011\n",
      "r2:  -0.0345\n",
      "MAE:  17.7736\n",
      "MSE:  8462.3367\n",
      "RMSE:  91.991\n",
      "\n",
      "\n",
      "explained_variance:  0.007\n",
      "r2:  -0.0246\n",
      "MAE:  11.9087\n",
      "MSE:  4143.7624\n",
      "RMSE:  64.3721\n",
      "\n",
      "\n",
      "SVM: STM\n",
      "explained_variance:  -0.0013\n",
      "r2:  -0.0353\n",
      "MAE:  17.7412\n",
      "MSE:  8468.8769\n",
      "RMSE:  92.0265\n",
      "\n",
      "\n",
      "explained_variance:  0.0072\n",
      "r2:  -0.0242\n",
      "MAE:  11.8028\n",
      "MSE:  4142.2328\n",
      "RMSE:  64.3602\n",
      "\n",
      "\n",
      "SVM: FWI\n",
      "explained_variance:  -0.0016\n",
      "r2:  -0.0365\n",
      "MAE:  17.8915\n",
      "MSE:  8479.0357\n",
      "RMSE:  92.0817\n",
      "\n",
      "\n",
      "explained_variance:  -0.001\n",
      "r2:  -0.0363\n",
      "MAE:  12.6287\n",
      "MSE:  4191.2103\n",
      "RMSE:  64.7396\n",
      "\n",
      "\n",
      "SVM: M\n",
      "explained_variance:  -0.0005\n",
      "r2:  -0.0357\n",
      "MAE:  17.9333\n",
      "MSE:  8472.113\n",
      "RMSE:  92.0441\n",
      "\n",
      "\n",
      "explained_variance:  0.0017\n",
      "r2:  -0.0333\n",
      "MAE:  12.5614\n",
      "MSE:  4178.8643\n",
      "RMSE:  64.6441\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 10, shuffle = True, random_state = 0)\n",
    "df_target = df['area_t'].round().astype(int)\n",
    "\n",
    "for idx, features in enumerate(feature_list):\n",
    "    print('SVM: ' + features_names[idx])\n",
    "    features = preprocessing.scale(features)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, df_target, test_size=0.3, random_state = 0 )\n",
    "    parameters = {'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001, .00001], 'kernel': ['rbf']} \n",
    "    grid = GridSearchCV(svm_func, parameters, cv=kf, scoring = 'neg_mean_squared_error')\n",
    "    grid.fit(X_train, y_train)\n",
    "    model = grid.best_estimator_\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = (np.exp(y_pred))-1\n",
    "    y_pred[y_pred < 0] = 0\n",
    "    y_test = (np.exp(y_test))-1\n",
    "    y_test[y_test < 0] = 0\n",
    "    y_true = y_test\n",
    "    regression_results(y_true, y_pred, 'svm')\n",
    "    print('\\n')\n",
    "    y_pred = model.predict(features)\n",
    "    y_pred = (np.exp(y_pred))-1\n",
    "    y_pred[y_pred < 0] = 0\n",
    "    y_true = df['area']\n",
    "    regression_results(y_true, y_pred, 'svm')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest and neural network code blocks are in markdown to avoid running them incidentally.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kf = KFold(n_splits = 10, shuffle = True, random_state = 0)\n",
    "df_target = df['area_t'].round().astype(int)\n",
    "\n",
    "#identify which parameters and ranges to include in the grid search\n",
    "#technique sourced from https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "print(rf.get_params())\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(train_features, train_labels)\n",
    "\n",
    "rf_random.best_params_\n",
    "\n",
    "#use a range around the best parameters as the parameter grid in the grid search\n",
    "\n",
    "for idx, features in enumerate(feature_list):\n",
    "    print('RF: ' + features_names[idx])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, df_target, test_size=0.3, random_state = 0 )\n",
    "    parameters = {}\n",
    "    grid = GridSearchCV(rf, parameters, cv=kf, scoring = 'neg_mean_absolute_error')\n",
    "    grid.fit(X_train, y_train)\n",
    "    model = grid.best_estimator_\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = (np.exp(y_pred))-1\n",
    "    y_pred[y_pred < 0] = 0\n",
    "    y_test = (np.exp(y_test))-1\n",
    "    y_test[y_test < 0] = 0\n",
    "    y_true = y_test\n",
    "    regression_results(y_true, y_pred, 'reg')\n",
    "    print('\\n')\n",
    "    y_pred = model.predict(features)\n",
    "    y_pred = (np.exp(y_pred))-1\n",
    "    y_pred[y_pred < 0] = 0\n",
    "    y_true = df['area']\n",
    "    regression_results(y_true, y_pred, 'reg')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kf = KFold(n_splits = 10, shuffle = True, random_state = 0)\n",
    "df_target = df['area_t'].round().astype(int)\n",
    "\n",
    "#identify which parameters and ranges to include in the grid search\n",
    "#technique sourced from https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "print(nn.get_params())\n",
    "\n",
    "#replicate the random forest example for identifying the range of best parameters to search for using the neural network parameters\n",
    "\n",
    "\n",
    "for idx, features in enumerate(feature_list):\n",
    "    print('NN: ' + features_names[idx])\n",
    "    features = preprocessing.scale(features)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, df_target, test_size=0.3, random_state = 0 )\n",
    "    parameters = {}\n",
    "    grid = GridSearchCV(nn, parameters, cv=kf, scoring = 'neg_mean_absolute_error')\n",
    "    grid.fit(X_train, y_train)\n",
    "    model = grid.best_estimator_\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = (np.exp(y_pred))-1\n",
    "    y_pred[y_pred < 0] = 0\n",
    "    y_test = (np.exp(y_test))-1\n",
    "    y_test[y_test < 0] = 0\n",
    "    y_true = y_test\n",
    "    regression_results(y_true, y_pred, 'reg')\n",
    "    print('\\n')\n",
    "    y_pred = model.predict(features)\n",
    "    y_pred = (np.exp(y_pred))-1\n",
    "    y_pred[y_pred < 0] = 0\n",
    "    y_true = df['area']\n",
    "    regression_results(y_true, y_pred, 'reg')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
